# -*- coding: utf-8 -*-
"""
Tokenization help for Adl programs.

generate_tokens() is a generator that breaks a stream of

text into Python tokens.  is called repeatedly to get the next token.

It generates tuple with these members:
    - the token type (see token.py)
    - the token (a string)
    - the offset of the token
    - the lineno of the token
    - the path of the source
"""


from delia_commons.exceptions import StopTokenizing
from .token import *
from .ctokenize import ScannerFromFile, ScannerFromString  # pylint: disable-msg=E0611


def get_type(typ, token, offset, lineno, col, path=None, macros=[]):
    return typ


def get_token(typ, token, offset, lineno, col, path=None, macros=[]):
    return token


def get_offset(typ, token, offset, lineno, col, path=None, macros=[]):
    return offset


def get_lineno(typ, token, offset, lineno, col, path=None, macros=[]):
    return lineno


def get_column(typ, token, offset, lineno, col, path=None, macros=[]):
    return col


def get_path(typ, token, offset, lineno, col, path=None, macros=[]):
    return path


def get_macro_stack(typ, token, offset, lineno, col, path=None, macros=[]):
    return macros


def get_tok_name(typ, token, offset, lineno, col, path=None, macros=[]):
    return tok_name[typ]


def get_toklist():
    return tok_name.items()


def iterator_to_scanner(iterator, path="<String>"):
        class Scanner:
            def __init__(self, iterator, path):
                self.path = path
                self.iterator = iterator

            def __iter__(self):
                return self

            def __next__(self):
                return next(self.iterator)

        return Scanner(iterator, path)


def tokenize_buf(buf):
    """
    Converts a string containing ADL source code to tokens list.
    """
    scanner = ScannerFromString(buf)
    return list(generate_tokens(scanner))


def tokenize(name, path, skip_ws=True):
    """
    The same as tokenizer_buf(open(path))
    """
    scanner = ScannerFromFile(path, skip_ws)
    return list(generate_tokens(scanner))


def generate_tokens(scanner):
    """
    The generate_tokens() generator.

    The generator produces tuple with these members: the token type; the
    token string; the offset of the token; the row indice of the token.
    """
    for token_infos in iter(scanner):
        yield token_infos


def printtoken(typ, token, offset, lineno, path):  # for testing
    print("%s,(%d,%d):\t%s\t%s" % (path, lineno, offset, tok_name[typ], repr(token)))


def tokenize_readlines(readlines, tokeneater=printtoken):
    """
    The tokenize_readlines() function accepts two parameters: one representing the
    input stream, and one providing an output mechanism for tokenize().

    The first parameter, readlines, must be a callable object which provides
    the same interface as the readlines() method of built-in file objects.
    Each call to the function should return one line of input as a string.

    The second parameter, tokeneater, must also be a callable object. It is
    called once for each token, with five arguments, corresponding to the
    tuples generated by generate_tokens().
    """
    try:
        for line in readlines():
            for token_info in tokenize(line):
                tokeneater(*token_info)
    except StopTokenizing:
        pass


if __name__ == '__main__':  # testing
    import sys
    if len(sys.argv) > 1:
        tokenize_readlines(open(sys.argv[1]).readlines)
    else:
        tokenize_readlines(sys. stdin.readlines)
